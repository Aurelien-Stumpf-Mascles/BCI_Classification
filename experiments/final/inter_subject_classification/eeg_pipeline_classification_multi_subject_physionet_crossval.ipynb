{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aurelien.stumpf/Development/BCI_Classification/eeg_env/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "import matplotlib.pyplot as plt\n",
    "import mne.viz\n",
    "import os\n",
    "import os.path as op\n",
    "from mne.preprocessing import (ICA, create_eog_epochs, create_ecg_epochs,\n",
    "                               corrmap)\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "from mne.datasets import fetch_fsaverage\n",
    "from mne import Epochs, EvokedArray, create_info, io, pick_types, read_events\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import Vectorizer\n",
    "from mne.decoding import CSP\n",
    "\n",
    "import torch\n",
    "from torcheeg.models import EEGNet\n",
    "from torcheeg.models import DGCNN\n",
    "from torcheeg import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import scipy as sc\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBClassifier\n",
    "import torchaudio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/home/aurelien.stumpf/Development/BCI_Classification/\")\n",
    "from eeg_project_package import dataset, models, spectral_analysis, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(training)\n",
    "importlib.reload(models)\n",
    "importlib.reload(spectral_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Loading of Braccio Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the parameters\n",
    "subject = 1  # use data from subject 1\n",
    "runs = [6, 10, 14]  # use only hand and feet motor imagery runs\n",
    "\n",
    "#Get data and locate in to given path\n",
    "input_fname1 =  \"/home/aurelien.stumpf/Development/Datasets/physionet.org/files/eegmmidb/1.0.0/S001/S001R04.edf\"\n",
    "#Read raw data files where each file contains a run\n",
    "raws = read_raw_edf(input_fname1, preload=True)\n",
    "#Combine all loaded runs\n",
    "#raw_obj = concatenate_raws(raws)\n",
    "raw_obj = raws\n",
    "\n",
    "raw_data = raw_obj.get_data()\n",
    "events = mne.events_from_annotations(raw_obj,event_id='auto')\n",
    "\n",
    "print(\"Number of channels: \", str(len(raw_data)))\n",
    "print(\"Number of samples: \", str(len(raw_data)))\n",
    "\n",
    "#Plot epochs & PSD\n",
    "raw_obj.plot(duration=120, n_channels=15, scalings=dict(eeg=420e-6))\n",
    "raw_obj.plot_psd(average=True)\n",
    "\n",
    "# list of all channel names\n",
    "list_all_ch_names = raw_obj.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sampling frequency\n",
    "sfreq = raw_obj.info['sfreq']\n",
    "print('Sampling frequency:', sfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_channels = {'Fc5.' : 'FC5',\n",
    " 'Fc3.' : 'FC3',\n",
    " 'Fc1.' : 'FC1',\n",
    " 'Fcz.' : 'FCz',\n",
    " 'Fc2.' : 'FC2',\n",
    " 'Fc4.' : 'FC4',\n",
    " 'Fc6.' : 'FC6',\n",
    " 'C5..' : 'C5',\n",
    " 'C3..' : 'C3',\n",
    " 'C1..' : 'C1',\n",
    " 'Cz..' : 'Cz',\n",
    " 'C2..' : 'C2',\n",
    " 'C4..' : 'C4',\n",
    " 'C6..' : 'C6',\n",
    " 'Cp5.' : 'CP5',\n",
    " 'Cp3.' : 'CP3',\n",
    " 'Cp1.' : 'CP1',\n",
    " 'Cpz.' : 'CPz',\n",
    " 'Cp2.' : 'CP2',\n",
    " 'Cp4.' : 'CP4',\n",
    " 'Cp6.' : 'CP6',\n",
    " 'Fp1.' : 'Fp1',\n",
    " 'Fpz.' : 'Fpz',\n",
    " 'Fp2.' : 'Fp2',\n",
    " 'Af7.' : 'AF7',\n",
    " 'Af3.' : 'AF3',\n",
    " 'Afz.' : 'AFz',\n",
    " 'Af4.' : 'AF4',\n",
    " 'Af8.' : 'AF8',\n",
    " 'F7..' : 'F7',\n",
    " 'F5..' : 'F5',\n",
    " 'F3..' : 'F3',\n",
    " 'F1..' : 'F1',\n",
    " 'Fz..' : 'Fz',\n",
    " 'F2..' : 'F2',\n",
    " 'F4..' : 'F4',\n",
    " 'F6..' : 'F6',\n",
    " 'F8..' : 'F8',\n",
    " 'Ft7.' : 'FT7',\n",
    " 'Ft8.' : 'FT8',\n",
    " 'T7..' : 'T7',\n",
    " 'T8..' : 'T8',\n",
    " 'T9..' : 'T9',\n",
    " 'T10.' : 'T10',\n",
    " 'Tp7.' : 'TP7',\n",
    " 'Tp8.' : 'TP8',\n",
    " 'P7..' : 'P7',\n",
    " 'P5..' : 'P5',\n",
    " 'P3..' : 'P3',\n",
    " 'P1..' : 'P1',\n",
    " 'Pz..' : 'Pz',\n",
    " 'P2..' : 'P2',\n",
    " 'P4..' : 'P4',\n",
    " 'P6..' : 'P6',\n",
    " 'P8..' : 'P8',\n",
    " 'Po7.' : 'PO7',\n",
    " 'Po3.' : 'PO3',\n",
    " 'Poz.' : 'POz',\n",
    " 'Po4.' : 'PO4',\n",
    " 'Po8.' : 'PO8',\n",
    " 'O1..' : 'O1',\n",
    " 'Oz..' : 'Oz',\n",
    " 'O2..' : 'O2',\n",
    " 'Iz..' : 'Iz'}\n",
    "\n",
    "dict_channels_inv = dict_channels.copy()\n",
    "\n",
    "# Reverse dict\n",
    "dict_channels = {v: k for k, v in dict_channels.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Subject Classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(training)\n",
    "importlib.reload(models)\n",
    "importlib.reload(spectral_analysis)\n",
    "\n",
    "# Define the channels\n",
    "list_name_channels = [\"CP1\",\"CP3\",\"CP5\",\"C1\",\"C3\",\"C5\",\"C2\",\"CP2\",\"Cz\",\"FCz\",\"C4\",\"CP4\"]\n",
    "list_idx_channels = [list_all_ch_names.index(dict_channels[ch]) for ch in list_name_channels]\n",
    "\n",
    "# Define the labels\n",
    "labels = ['T0', 'T2']\n",
    "\n",
    "parent_folder_path = \"/home/aurelien.stumpf/Development/Datasets/physionet.org/files/eegmmidb/1.0.0/\"\n",
    "\n",
    "num_runs = [4, 8, 12]\n",
    "\n",
    "dict_crossval = {\"test_balanced_accuracy\":[], \"test_loss\":[], \"val_balanced_accuracy\":[], \"val_loss\":[], \"train_balanced_accuracy\":[], \"train_loss\":[]}\n",
    "\n",
    "K = 8\n",
    "num_subjects = list(range(0,109))\n",
    "for k in range(K-1):\n",
    "    print(k)\n",
    "\n",
    "    num_test_subjects = num_subjects[k*int(len(num_subjects)/K):(k+1)*int(len(num_subjects)/K)]\n",
    "    num_val_subjects = num_subjects[(k+1)*int(len(num_subjects)/K):(k+2)*int(len(num_subjects)/K)]\n",
    "    num_train_subjects = [x for x in num_subjects if x not in num_test_subjects and x not in num_val_subjects]\n",
    "    num_sessions = []\n",
    "    list_labels = [\"T0\",\"T2\"]\n",
    "\n",
    "    # Define the parameters of the dataset\n",
    "    print(\"Creating the dataset\")\n",
    "\n",
    "    feature_type = [\"time\"]\n",
    "    dict_preprocessing = {\"polynomial_degree\":None,\"tmin\":0,\"tmax\":4,\"scale\":\"standard\",\"seq_length\":641,\"fs\":160}\n",
    "    dict_features = {\"type_psd\":\"welch\",\"fs\":500,\"nfft\":300,\"noverlap\":150,\"nperseg\":300,\"filter_order\":19,\"fmin\":4,\"fmax\":30}\n",
    "\n",
    "    print(\"training set\")\n",
    "    torch_trainset = dataset.Physio_Dataset_Multi_Subject(parent_folder_path, num_train_subjects, num_runs, list_idx_channels, list_labels, feature_type, dict_preprocessing, dict_features)\n",
    "    print(\"val set\")\n",
    "    torch_valset = dataset.Physio_Dataset_Multi_Subject(parent_folder_path, num_val_subjects, num_runs, list_idx_channels, list_labels, feature_type, dict_preprocessing, dict_features)\n",
    "    print(\"test set\")\n",
    "    torch_testset = dataset.Physio_Dataset_Multi_Subject(parent_folder_path, num_test_subjects, num_runs, list_idx_channels, list_labels, feature_type, dict_preprocessing, dict_features)\n",
    "\n",
    "    torch_trainset.transform_dataset_numpy_to_torch()\n",
    "    torch_trainset.features[\"time\"] = torch_trainset.features[\"time\"].unsqueeze(1)\n",
    "    torch_valset.transform_dataset_numpy_to_torch()\n",
    "    torch_valset.features[\"time\"] = torch_valset.features[\"time\"].unsqueeze(1)\n",
    "    torch_testset.transform_dataset_numpy_to_torch()\n",
    "    torch_testset.features[\"time\"] = torch_testset.features[\"time\"].unsqueeze(1)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(torch_trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    valloader = torch.utils.data.DataLoader(torch_valset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(torch_testset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "    print(\"Start the training\")\n",
    "\n",
    "    feature_type = \"time\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model = EEGNet(chunk_size=641,\n",
    "                num_electrodes=12,\n",
    "                dropout=0.25,\n",
    "                kernel_1=50,\n",
    "                kernel_2=25,\n",
    "                F1=10,\n",
    "                F2=16,\n",
    "                D=2,\n",
    "                num_classes=2)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    #scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 50], gamma=0.1)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.97)\n",
    "    scheduler_dict = {\"MultiplicativeLR\" : scheduler}\n",
    "    logs = training.train_model(model, trainloader, testloader, device, criterion, feature_type, 51, optimizer, scheduler_dict, print_epoch = 5)\n",
    "    \n",
    "    # change weights of the model to the best weights\n",
    "    model.load_state_dict(logs[\"best_model_weights\"])\n",
    "    model.eval()\n",
    "    train_balanced_accuracy, train_loss = training.evaluate_classification_model(model,trainloader,device,criterion,feature_type,dataset=\"Train\")\n",
    "    val_balanced_accuracy, val_loss = training.evaluate_classification_model(model,valloader,device,criterion,feature_type,dataset=\"Val\")\n",
    "    test_balanced_accuracy, test_loss = training.evaluate_classification_model(model,testloader,device,criterion,feature_type,dataset=\"Test\")\n",
    "    dict_crossval[\"test_balanced_accuracy\"].append(test_balanced_accuracy)\n",
    "    dict_crossval[\"test_loss\"].append(test_loss)\n",
    "    dict_crossval[\"val_balanced_accuracy\"].append(val_balanced_accuracy)\n",
    "    dict_crossval[\"val_loss\"].append(val_loss)\n",
    "    dict_crossval[\"train_balanced_accuracy\"].append(train_balanced_accuracy)\n",
    "    dict_crossval[\"train_loss\"].append(train_loss)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_crossval[\"test_balanced_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../dicts_results/inter_subject_classification_physionet/dict_crossval_12_subjects_8_folds.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_crossval, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(training)\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(training)\n",
    "importlib.reload(models)\n",
    "importlib.reload(spectral_analysis)\n",
    "\n",
    "# Define the channels\n",
    "list_name_channels = [\"CP1\",\"CP3\",\"CP5\",\"C1\",\"C3\",\"C5\",\"C2\",\"CP2\",\"Cz\",\"FCz\",\"C4\",\"CP4\"]\n",
    "list_idx_channels = [list_all_ch_names.index(dict_channels[ch]) for ch in list_name_channels]\n",
    "\n",
    "# Define the labels\n",
    "labels = ['T0', 'T2']\n",
    "\n",
    "parent_folder_path = \"/home/aurelien.stumpf/Development/Datasets/physionet.org/files/eegmmidb/1.0.0/\"\n",
    "\n",
    "num_runs = [4, 8, 12]\n",
    "\n",
    "#dict_crossval = {\"test_balanced_accuracy\":[], \"test_loss\":[], \"val_balanced_accuracy\":[], \"val_loss\":[], \"train_balanced_accuracy\":[], \"train_loss\":[]}\n",
    "\n",
    "K = 8\n",
    "num_subjects = list(range(0,109))\n",
    "for k in range(2,K-1):\n",
    "    print(k)\n",
    "\n",
    "    num_test_subjects = num_subjects[k*int(len(num_subjects)/K):(k+1)*int(len(num_subjects)/K)]\n",
    "    num_val_subjects = num_subjects[(k+1)*int(len(num_subjects)/K):(k+2)*int(len(num_subjects)/K)]\n",
    "    num_train_subjects = [x for x in num_subjects if x not in num_test_subjects and x not in num_val_subjects]\n",
    "    num_sessions = []\n",
    "    list_labels = [\"T0\",\"T2\"]\n",
    "\n",
    "    # Define the parameters of the dataset\n",
    "    print(\"Creating the dataset\")\n",
    "\n",
    "    feature_type = [\"time\"]\n",
    "    dict_preprocessing = {\"polynomial_degree\":None,\"tmin\":0,\"tmax\":4,\"scale\":\"standard\",\"seq_length\":641,\"fs\":160}\n",
    "    dict_features = {\"type_psd\":\"welch\",\"fs\":500,\"nfft\":300,\"noverlap\":150,\"nperseg\":300,\"filter_order\":19,\"fmin\":4,\"fmax\":30}\n",
    "\n",
    "    print(\"training set\")\n",
    "    torch_trainset = dataset.Physio_Dataset_Multi_Subject(parent_folder_path, num_train_subjects, num_runs, list_idx_channels, list_labels, feature_type, dict_preprocessing, dict_features)\n",
    "    print(\"val set\")\n",
    "    torch_valset = dataset.Physio_Dataset_Multi_Subject(parent_folder_path, num_val_subjects, num_runs, list_idx_channels, list_labels, feature_type, dict_preprocessing, dict_features)\n",
    "    print(\"test set\")\n",
    "    torch_testset = dataset.Physio_Dataset_Multi_Subject(parent_folder_path, num_test_subjects, num_runs, list_idx_channels, list_labels, feature_type, dict_preprocessing, dict_features)\n",
    "\n",
    "    torch_trainset.transform_dataset_numpy_to_torch()\n",
    "    torch_trainset.features[\"time\"] = torch_trainset.features[\"time\"].unsqueeze(1)\n",
    "    torch_valset.transform_dataset_numpy_to_torch()\n",
    "    torch_valset.features[\"time\"] = torch_valset.features[\"time\"].unsqueeze(1)\n",
    "    torch_testset.transform_dataset_numpy_to_torch()\n",
    "    torch_testset.features[\"time\"] = torch_testset.features[\"time\"].unsqueeze(1)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(torch_trainset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    valloader = torch.utils.data.DataLoader(torch_valset, batch_size=16, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(torch_testset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "    print(\"Start the training\")\n",
    "\n",
    "    feature_type = \"time\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model = models.ATCNet(in_channels=1,\n",
    "                num_classes=2,\n",
    "                num_windows=5,\n",
    "                num_electrodes=12,\n",
    "                chunk_size=641,\n",
    "                filter_size=50,)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler1 = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.95)\n",
    "    scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "    scheduler_dict = {\"MultiplicativeLR\":scheduler1} #, \"ReduceLROnPlateau\":scheduler2}\n",
    "    logs = training.train_model(model, trainloader, testloader, device, criterion, feature_type, 51, optimizer, scheduler_dict, print_epoch = 5)\n",
    "    \n",
    "    # change weights of the model to the best weights\n",
    "    model.load_state_dict(logs[\"best_model_weights\"])\n",
    "    model.eval()\n",
    "    train_balanced_accuracy, train_loss = training.evaluate_classification_model(model,trainloader,device,criterion,feature_type,dataset=\"Train\")\n",
    "    val_balanced_accuracy, val_loss = training.evaluate_classification_model(model,valloader,device,criterion,feature_type,dataset=\"Val\")\n",
    "    test_balanced_accuracy, test_loss = training.evaluate_classification_model(model,testloader,device,criterion,feature_type,dataset=\"Test\")\n",
    "    dict_crossval[\"test_balanced_accuracy\"].append(test_balanced_accuracy)\n",
    "    dict_crossval[\"test_loss\"].append(test_loss)\n",
    "    dict_crossval[\"val_balanced_accuracy\"].append(val_balanced_accuracy)\n",
    "    dict_crossval[\"val_loss\"].append(val_loss)\n",
    "    dict_crossval[\"train_balanced_accuracy\"].append(train_balanced_accuracy)\n",
    "    dict_crossval[\"train_loss\"].append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = \"time\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = models.ATCNet(in_channels=1,\n",
    "               num_classes=2,\n",
    "               num_windows=5,\n",
    "               num_electrodes=12,\n",
    "               chunk_size=641,\n",
    "               filter_size=50,)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler1 = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.95)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "scheduler_dict = {\"MultiplicativeLR\":scheduler1} #, \"ReduceLROnPlateau\":scheduler2}\n",
    "logs = training.train_model(model, trainloader, testloader, device, criterion, feature_type, 101, optimizer, scheduler_dict, print_epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_eegnet = np.array([0.75, 0.75, 0.77])\n",
    "li_atcnet = np.array([0.77, 0.76, 0.78])\n",
    "print(np.mean(li_eegnet), np.mean(li_atcnet))\n",
    "print(np.std(li_eegnet), np.std(li_atcnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.data.shape)\n",
    "    time_filters = param.data.cpu().numpy()\n",
    "    break\n",
    "\n",
    "for i in range(16):\n",
    "    plt.plot(time_filters[i,0,0,:])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        spatial_map_12 = model.conv_block[2].weight.data[i + 4 * j].cpu().numpy().flatten()\n",
    "        spatial_map_64 = np.zeros(64)\n",
    "        spatial_map_64[list_idx_channels] = spatial_map_12\n",
    "        mne.viz.plot_topomap(\n",
    "            spatial_map_64,\n",
    "            ch_positions[:, :2],\n",
    "            show=False,\n",
    "            axes=ax[i, j]\n",
    "        )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA / SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch_trainset.features[\"band_psd\"].reshape(torch_trainset.features[\"band_psd\"].shape[0], -1)\n",
    "y_train = torch_trainset.labels\n",
    "X_test = torch_testset.features[\"band_psd\"].reshape(torch_testset.features[\"band_psd\"].shape[0], -1)\n",
    "y_test = torch_testset.labels\n",
    "\n",
    "# Train LDA classifier\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "lda_clf.fit(X_train, y_train)\n",
    "y_pred = lda_clf.predict(X_test)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"LDA accuracy: \", acc)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"SVM accuracy: \", acc)\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_clf = SVC(kernel='rbf')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"rbf SVM accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_trainset.features[\"time\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(training)\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_trainset.transform_dataset_numpy_to_torch()\n",
    "torch_trainset.features[\"time\"] = torch_trainset.features[\"time\"][:,0,:].squeeze(1)\n",
    "torch_testset.transform_dataset_numpy_to_torch()\n",
    "torch_testset.features[\"time\"] = torch_testset.features[\"time\"][:,0,:].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = \"time\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = torch.nn.BCEWithLogitsLoss() #CrossEntropyLoss()\n",
    "model = models.DeepWelchTransform(nperseg = 400, noverlap = 100, seq_length = 641)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler1 = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda epoch: 0.95)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "scheduler_dict = {\"MultiplicativeLR\":scheduler1} #, \"ReduceLROnPlateau\":scheduler2}\n",
    "logs = training.train_model(model, trainloader, testloader, device, criterion, feature_type, 101, optimizer, scheduler_dict, print_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
